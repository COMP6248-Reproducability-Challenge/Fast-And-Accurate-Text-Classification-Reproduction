{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dl_reproducibility.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/comp6248-polaris/reproducibility_challenge/blob/master/dl_reproducibility.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "LhjsbQ2aIb2Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import optim\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Bernoulli, Multinomial\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "from torchtext import datasets\n",
        "from torchtext import data\n",
        "\n",
        "import os\n",
        "import numpy as np \n",
        "import random\n",
        "from itertools import count\n",
        "import collections\n",
        "from tqdm import  tqdm\n",
        "\n",
        "from sklearn.metrics import f1_score\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u6D1DNZAXEBo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "metadata": {
        "id": "0tG7-T-LJBbQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    \n",
        "SEED = 2019\n",
        "set_seed(SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6E7uX9ixWeE3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TEXT = data.Field(sequential=True, tokenize='spacy', lower=True, fix_length=400) \n",
        "LABEL = data.LabelField(dtype=torch.float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "18c67kXhWip1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b44cb210-a4fc-4ed3-d845-d4d1e5ab19f3"
      },
      "cell_type": "code",
      "source": [
        "train, test_data = datasets.IMDB.splits(TEXT, LABEL) # 25,000 training and 25,000 testing data\n",
        "train_data, valid_data = train.split(split_ratio=0.8) # split training data into 20,000 training and 5,000 vlidation sample"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:07<00:00, 10.9MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "CU7qHlVaWtLG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "00b5c760-355e-4ae7-b9fb-57488dda6a78"
      },
      "cell_type": "code",
      "source": [
        "MAX_VOCAB_SIZE = 25_000\n",
        "\n",
        "TEXT.build_vocab(train_data, max_size=MAX_VOCAB_SIZE, vectors=\"glove.6B.100d\", unk_init = torch.Tensor.normal_)\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [00:54, 15.9MB/s]                           \n",
            "100%|█████████▉| 398983/400000 [00:19<00:00, 20372.74it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "HRk_VHSgWmfs",
        "colab_type": "code",
        "outputId": "915c9865-8afb-4269-94e1-813d171b0a4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of validation examples: {len(valid_data)}')\n",
        "print(f'Number of testing examples: {len(test_data)}')\n",
        "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
        "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 20000\n",
            "Number of validation examples: 5000\n",
            "Number of testing examples: 25000\n",
            "Unique tokens in TEXT vocabulary: 25002\n",
            "Unique tokens in LABEL vocabulary: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mhdD94kKW3zh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 1\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size=BATCH_SIZE,\n",
        "    device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iPs55SMqXJbJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Networks architecture"
      ]
    },
    {
      "metadata": {
        "id": "zxC2Z0nBe7OL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CNN_LSTM(nn.Module):\n",
        "  \n",
        "    def __init__(self,input_dim, embedding_dim, ker_size, n_filters, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        # self.embedding.weight.requires_grad = False\n",
        "        self.conv = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(ker_size, embedding_dim))\n",
        "        self.lstm = nn.LSTM(n_filters*16, hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, text): # input 1 by 20\n",
        "        # CNN and LSTM network\n",
        "        '''\n",
        "        \n",
        "        --- input & output dimension ---\n",
        "        \n",
        "        Input text: \n",
        "        \n",
        "        **Embedding**\n",
        "        1.Input:\n",
        "        2.Output:\n",
        "        \n",
        "        **CNN**\n",
        "        1. Input(minibatch×in_channels×iH×iW):\n",
        "        2. Output:\n",
        "        \n",
        "        **LSTM**\n",
        "        1. Inputs: input, (h_0, c_0)\n",
        "        input(seq_len, batch, input_size):\n",
        "        h_0:\n",
        "        2. Outputs: output, (h_n, c_n)\n",
        "        output:\n",
        "        h_n(num_layers * num_directions, batch, hidden_size):\n",
        "        \n",
        "\n",
        "        '''\n",
        "        embedded = self.embedding(text)\n",
        "        conved = self.conv(embedded.unsqueeze(1))\n",
        "        conved = F.relu(conved)\n",
        "        conved = conved.squeeze(3) # conved is 1*128 * 16\n",
        "        staked = conved.view(1, 128*16)\n",
        "#         staked = staked.unsqueeze(1)\n",
        "        packed_output, (hn,cn) = self.lstm(staked) \n",
        "        out = self.fc(hn.squeeze(0)) # input 1*128\n",
        "        return out\n",
        "    \n",
        "class Policy_S(nn.Module):\n",
        "    def __init__(self, hidden_dim, output_dim):\n",
        "        super().__init__()    \n",
        "        self.fc_s_hidden0 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc_s_hidden1 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc_s_hidden2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc_s_output = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    \n",
        "    def forward(self, ht):\n",
        "        # pi_s\n",
        "        out = self.fc_s_hidden0(ht)\n",
        "        out = self.fc_s_hidden1(out)\n",
        "        out = self.fc_s_hidden2(out)\n",
        "        s = self.fc_s_output(out)\n",
        "        s_pro = torch.sigmoid(s)\n",
        "        return s_pro\n",
        "\n",
        "class Policy_C(nn.Module):\n",
        "  \n",
        "    def __init__(self, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.fc_c_hidden = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc_c_output = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, ht):\n",
        "        # pi_c\n",
        "        out = self.fc_c_hidden(ht)\n",
        "        c = self.fc_c_output(out)\n",
        "\n",
        "        return c\n",
        "  \n",
        "  \n",
        "class Policy_N(nn.Module):\n",
        "  \n",
        "    def __init__(self, hidden_dim, max_k):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.fc_n_hidden0 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc_n_hidden1 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc_n_hidden2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc_n_output = nn.Linear(hidden_dim, max_k+1)\n",
        "\n",
        "    def forward(self, ht):\n",
        "\n",
        "       \n",
        "        out = self.fc_n_hidden0(ht)\n",
        "        out = self.fc_n_hidden1(out)\n",
        "        out = self.fc_n_hidden2(out)\n",
        "        n = self.fc_n_output(out)\n",
        "        n_pro = torch.softmax(n,1)\n",
        "    \n",
        "        return n_pro\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9GRm9gcTslK2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import torchvision.models as models\n",
        "\n",
        "# def print_model_parm_flops(model, input):\n",
        "\n",
        "#     # prods = {}\n",
        "#     # def save_prods(self, input, output):\n",
        "#         # print 'flops:{}'.format(self.__class__.__name__)\n",
        "#         # print 'input:{}'.format(input)\n",
        "#         # print '_dim:{}'.format(input[0].dim())\n",
        "#         # print 'input_shape:{}'.format(np.prod(input[0].shape))\n",
        "#         # grads.append(np.prod(input[0].shape))\n",
        "\n",
        "#     prods = {}\n",
        "#     def save_hook(name):\n",
        "#         def hook_per(self, input, output):\n",
        "#             # print 'flops:{}'.format(self.__class__.__name__)\n",
        "#             # print 'input:{}'.format(input)\n",
        "#             # print '_dim:{}'.format(input[0].dim())\n",
        "#             # print 'input_shape:{}'.format(np.prod(input[0].shape))\n",
        "#             # prods.append(np.prod(input[0].shape))\n",
        "#             prods[name] = np.prod(input[0].shape)\n",
        "#             # prods.append(np.prod(input[0].shape))\n",
        "#         return hook_per\n",
        "\n",
        "#     list_1=[]\n",
        "#     def simple_hook(self, input, output):\n",
        "#         list_1.append(np.prod(input[0].shape))\n",
        "#     list_2={}\n",
        "#     def simple_hook2(self, input, output):\n",
        "#         list_2['names'] = np.prod(input[0].shape)\n",
        "\n",
        "\n",
        "#     multiply_adds = False\n",
        "#     list_conv=[]\n",
        "#     def conv_hook(self, input, output):\n",
        "#         batch_size, input_channels, input_height, input_width = input[0].size()\n",
        "#         output_channels, output_height, output_width = output[0].size()\n",
        "\n",
        "#         kernel_ops = self.kernel_size[0] * self.kernel_size[1] * (self.in_channels / self.groups) * (2 if multiply_adds else 1)\n",
        "#         bias_ops = 1 if self.bias is not None else 0\n",
        "\n",
        "#         params = output_channels * (kernel_ops + bias_ops)\n",
        "#         flops = batch_size * params * output_height * output_width\n",
        "\n",
        "#         list_conv.append(flops)\n",
        "\n",
        "\n",
        "#     list_linear=[] \n",
        "#     def linear_hook(self, input, output):\n",
        "#         batch_size = input[0].size(0) if input[0].dim() == 2 else 1\n",
        "\n",
        "#         weight_ops = self.weight.nelement() * (2 if multiply_adds else 1)\n",
        "#         bias_ops = self.bias.nelement()\n",
        "\n",
        "#         flops = batch_size * (weight_ops + bias_ops)\n",
        "#         list_linear.append(flops)\n",
        "\n",
        "#     list_bn=[] \n",
        "#     def bn_hook(self, input, output):\n",
        "#         list_bn.append(input[0].nelement())\n",
        "\n",
        "#     list_relu=[] \n",
        "#     def relu_hook(self, input, output):\n",
        "#         list_relu.append(input[0].nelement())\n",
        "\n",
        "#     list_pooling=[]\n",
        "#     def pooling_hook(self, input, output):\n",
        "#         batch_size, input_channels, input_height, input_width = input[0].size()\n",
        "#         output_channels, output_height, output_width = output[0].size()\n",
        "\n",
        "#         kernel_ops = self.kernel_size * self.kernel_size\n",
        "#         bias_ops = 0\n",
        "#         params = output_channels * (kernel_ops + bias_ops)\n",
        "#         flops = batch_size * params * output_height * output_width\n",
        "\n",
        "#         list_pooling.append(flops)\n",
        "\n",
        "\n",
        "            \n",
        "#     def foo(net):\n",
        "#         childrens = list(net.children())\n",
        "#         if not childrens:\n",
        "#             if isinstance(net, torch.nn.Conv2d):\n",
        "#                 # net.register_forward_hook(save_hook(net.__class__.__name__))\n",
        "#                 # net.register_forward_hook(simple_hook)\n",
        "#                 # net.register_forward_hook(simple_hook2)\n",
        "#                 net.register_forward_hook(conv_hook)\n",
        "#             if isinstance(net, torch.nn.Linear):\n",
        "#                 net.register_forward_hook(linear_hook)\n",
        "#             if isinstance(net, torch.nn.BatchNorm2d):\n",
        "#                 net.register_forward_hook(bn_hook)\n",
        "#             if isinstance(net, torch.nn.ReLU):\n",
        "#                 net.register_forward_hook(relu_hook)\n",
        "#             if isinstance(net, torch.nn.MaxPool2d) or isinstance(net, torch.nn.AvgPool2d):\n",
        "#                 net.register_forward_hook(pooling_hook)\n",
        "#             return\n",
        "#         for c in childrens:\n",
        "#                 foo(c)\n",
        "\n",
        "#     foo(model)\n",
        "#     out = model(input.cuda())\n",
        "\n",
        "\n",
        "#     total_flops = (sum(list_conv) + sum(list_linear) + sum(list_bn) + sum(list_relu) + sum(list_pooling))\n",
        "    \n",
        "# #     print('  + Number of FLOPs: {0}'.format (total_flops))\n",
        "#     return total_flops\n",
        "#     # print list_bn\n",
        "\n",
        "\n",
        "#     # print 'prods:{}'.format(prods)\n",
        "#     # print 'list_1:{}'.format(list_1)\n",
        "#     # print 'list_2:{}'.format(list_2)\n",
        "#     # print 'list_final:{}'.format(list_final)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t5d9i6w1YgCd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# INPUT_DIM = len(TEXT.vocab)\n",
        "# EMBEDDING_DIM = 100\n",
        "# KER_SIZE = 5\n",
        "# HIDDEN_DIM = 128\n",
        "# OUTPUT_DIM = 1\n",
        "# CHUNCK_SIZE = 20\n",
        "# TEXT_LEN = 400\n",
        "# MAX_K = 3\n",
        "\n",
        "# BATCH_SIZE = 1\n",
        "\n",
        "# learning_rate = 0.001\n",
        "\n",
        "# test_model = CNN_RNN(INPUT_DIM, EMBEDDING_DIM, KER_SIZE, HIDDEN_DIM).train().cuda()\n",
        "# test_policy_s = Policy_S(HIDDEN_DIM, OUTPUT_DIM).train().cuda()\n",
        "# test_policy_n = Policy_N(HIDDEN_DIM, MAX_K).train().cuda()\n",
        "# test_policy_c = Policy_C(HIDDEN_DIM, OUTPUT_DIM).train().cuda()\n",
        "\n",
        "# cnn_cost = -print_model_parm_flops(test_model, torch.randint(1,100, (1, 20)))\n",
        "# p = torch.rand(1,128)\n",
        "# s_cost = -print_model_parm_flops(test_policy_s, p)\n",
        "# c_cost = -print_model_parm_flops(test_policy_c, p)\n",
        "# n_cost = -print_model_parm_flops(test_policy_n, p)\n",
        "# print('cnn_cost', cnn_cost)\n",
        "# print('s_cost', s_cost)\n",
        "# print('c_cost', c_cost)\n",
        "# print('n_cost', n_cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DCCWLXjNw7lL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "s_cost = -66177\n",
        "c_cost = -33153\n",
        "n_cost = -66564\n",
        "cnn_cost = -1026048.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KEF1MIgIlghi",
        "colab_type": "code",
        "outputId": "9e3b1870-fc46-4f9a-da0b-d7d06f548f4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3400
        }
      },
      "cell_type": "code",
      "source": [
        "print(len(TEXT.vocab))\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "KER_SIZE = 5\n",
        "HIDDEN_DIM = 128\n",
        "OUTPUT_DIM = 1\n",
        "CHUNCK_SIZE = 20\n",
        "TEXT_LEN = 400\n",
        "MAX_K = 3\n",
        "\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "gamma = 0.99\n",
        "alpha = 0.2\n",
        "learning_rate = 0.001\n",
        "\n",
        "model = CNN_RNN(INPUT_DIM, EMBEDDING_DIM, KER_SIZE, HIDDEN_DIM)\n",
        "policy_s = Policy_S(HIDDEN_DIM, OUTPUT_DIM)\n",
        "policy_n = Policy_N(HIDDEN_DIM, MAX_K)\n",
        "policy_c = Policy_C(HIDDEN_DIM, OUTPUT_DIM)\n",
        "\n",
        "# model.cuda()\n",
        "# policy_s.cuda()\n",
        "# policy_n.cuda()\n",
        "# policy_c.cuda()\n",
        "\n",
        "loss_function = nn.BCEWithLogitsLoss()\n",
        "\n",
        "\n",
        "params = list(model.parameters()) + list(policy_s.parameters()) + list(policy_n.parameters()) + list(policy_c.parameters())\n",
        "optimizer = optim.Adam(params, lr=learning_rate)\n",
        "\n",
        "for epoch in range(1):\n",
        "    print('epoch', epoch)\n",
        "    print('train')\n",
        "    # train\n",
        "    model.train()\n",
        "    policy_s.train()\n",
        "    policy_n.train()\n",
        "    policy_c.train()\n",
        "\n",
        "    for index, (train) in enumerate(train_iterator):\n",
        "        text = train.text.transpose(0,1)\n",
        "        label = train.label\n",
        "\n",
        "        pre_label = 0\n",
        "        curr_step = 0\n",
        "        \n",
        "        text = text.view(CHUNCK_SIZE, BATCH_SIZE, CHUNCK_SIZE) # transform 1*400 to 20*1*20\n",
        "\n",
        "        state_pool = []\n",
        "        action_pool = []\n",
        "        reward_pool = []\n",
        "    \n",
        "        for t in count(): # loop until a text is classified or currstep is up to 20\n",
        "            reward = 0\n",
        "            cost = 0\n",
        "\n",
        "            if curr_step >= 20: # when curr step is beyond 0 - 19\n",
        "                break\n",
        "\n",
        "            text_input = text[curr_step] # text_input 1*20\n",
        "            \n",
        "            ht = model(text_input)\n",
        "            s_pro = policy_s(ht)\n",
        "            m = Bernoulli(s_pro.detach())\n",
        "            s_action = m.sample()\n",
        "            action = s_action.item()\n",
        "            cost += (cnn_cost + s_cost)\n",
        "\n",
        "            if int(action) == 1: # s_action is 1, then classify\n",
        "                c = policy_c(ht)\n",
        "                loss = loss_function(c.squeeze(1), label)\n",
        "                reward += (loss + alpha*cost)\n",
        "                state_pool.append(ht)\n",
        "                reward_pool.append(reward)\n",
        "                action_pool.append([s_action])\n",
        "                break\n",
        "\n",
        "            elif int(action) == 0: # s_action is 0, then compute next action \n",
        "                n_pro = policy_n(ht)\n",
        "                m = Multinomial(1, n_pro.detach())\n",
        "                n_action = m.sample()\n",
        "                step = torch.argmax(n_action)\n",
        "                curr_step += step\n",
        "                cost += n_cost\n",
        "                reward += alpha*cost\n",
        "\n",
        "\n",
        "            state_pool.append(ht)\n",
        "            reward_pool.append(reward)\n",
        "            action_pool.append([s_action, n_action])\n",
        "      \n",
        "    \n",
        "        # compute G with gamma, RINFORCE gradient descent is adopted here\n",
        "\n",
        "        running_add =0 \n",
        "        for i in reversed(range(len(reward_pool))):\n",
        "            if reward_pool[i] == 0:\n",
        "                running_add = 0\n",
        "            else:\n",
        "                running_add = running_add * gamma + reward_pool[i]\n",
        "                reward_pool[i] = running_add\n",
        "          \n",
        "    # update policy\n",
        "        optimizer.zero_grad()\n",
        "        sum_pro = torch.zeros([1,1])\n",
        "        policy_s.eval()\n",
        "        policy_c.eval()\n",
        "        policy_n.eval()\n",
        "        \n",
        "        for i in range(len(reward_pool)):\n",
        "            state = state_pool[i]\n",
        "            action = action_pool[i]\n",
        "            reward = reward_pool[i]\n",
        "\n",
        "            if i == len(reward_pool)-1: # the last time step \n",
        "                s_pro = policy_s(state)\n",
        "                m = Bernoulli(s_pro)\n",
        "                c = policy_c(state)\n",
        "                c_pro = torch.sigmoid(c)\n",
        "                sum_pro += (m.log_prob(action[0]) + torch.log(c_pro))\n",
        "            else:\n",
        "                s_pro = policy_s(state)\n",
        "                m = Bernoulli(s_pro)\n",
        "                n_pro = policy_n(state)\n",
        "                n = Multinomial(1, n_pro)\n",
        "\n",
        "                sum_pro += (m.log_prob(action[0]) + n.log_prob(action[1])) # Negtive score function x reward\n",
        "                \n",
        "        sum_loss = sum_pro*reward_pool[0]\n",
        "        sum_loss.backward()  \n",
        "            \n",
        "        optimizer.step()          \n",
        "#     print('index:{0}'.format(index))\n",
        "        if index == 50: # train on 100 training data\n",
        "            break\n",
        "\n",
        "  # eval\n",
        "    model.eval()\n",
        "    policy_s.eval()\n",
        "    policy_c.eval()\n",
        "    policy_n.eval()\n",
        "    print('eval')\n",
        "    \n",
        "    valid_labels = []\n",
        "    predicted_labels = []\n",
        "    \n",
        "    for index, (valid) in enumerate(valid_iterator):\n",
        "        text = valid.text.transpose(0,1)\n",
        "        label = valid.label.cpu()\n",
        "        curr_step = 0\n",
        "        text = text.view(CHUNCK_SIZE, BATCH_SIZE, CHUNCK_SIZE)\n",
        "    \n",
        "        for t in count():\n",
        "      \n",
        "            if curr_step >= 20:\n",
        "                break\n",
        "        \n",
        "            text_input = text[curr_step]\n",
        "            ht = model(text_input)\n",
        "            s_pro = policy_s(ht)\n",
        "            m = Bernoulli(s_pro.detach())\n",
        "            s_action = m.sample()\n",
        "            action = s_action\n",
        "      \n",
        "      \n",
        "            if int(action) == 1:\n",
        "                c = policy_c(ht)        \n",
        "                break\n",
        "            elif int(action) == 0:\n",
        "                n_pro = policy_n(ht)\n",
        "                m = Multinomial(1, n_pro.detach())\n",
        "                n_action = m.sample()\n",
        "                step = torch.argmax(n_action)\n",
        "                curr_step += step\n",
        "        \n",
        "        c_pro = torch.sigmoid(c)\n",
        "      \n",
        "        print('c_pro:{0}, pre_label:{1}, label:{2}'.format(c_pro, pre_label, label))\n",
        "        \n",
        "        valid_labels.append(label)\n",
        "        predicted_labels.append(c_pro)\n",
        "        if index == 2: # evaluate on 10 valid data\n",
        "            break\n",
        "      \n",
        "    \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25002\n",
            "epoch 0\n",
            "train\n",
            "s_pro tensor([[0.5131]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.5113]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.5132]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.4975]]) tensor(0.6982, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.4970]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.4970]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.5081]]) tensor(0.6770, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.4844]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.5082]]) tensor(0.6768, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.4945]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.4870]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.4892]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.4892]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.5360]]) tensor(0.7678, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.4808]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.5249]]) tensor(0.7442, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.4639]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.5280]]) tensor(0.7507, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.4659]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.4659]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.5371]]) tensor(0.6216, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.4526]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.4546]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.5555]]) tensor(0.5878, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.4560]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.5403]]) tensor(0.6156, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.4543]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.5447]]) tensor(0.7868, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.4602]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.4531]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.5491]]) tensor(0.7966, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.4504]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.6019]]) tensor(0.9210, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.4603]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.4775]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.4927]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.4927]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.6569]]) tensor(1.0698, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.4487]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.6386]]) tensor(0.4484, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.4471]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.5985]]) tensor(0.5133, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.4350]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.4350]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.6062]]) tensor(0.9320, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.4610]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.4737]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.4527]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.4527]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.4527]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.6046]]) tensor(0.9280, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.4504]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.4351]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.4251]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.4339]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.4298]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.6779]]) tensor(0.3887, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.4312]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.4312]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.6457]]) tensor(0.4374, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.4108]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.4299]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.4299]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.3971]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.6574]]) tensor(0.4194, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.4139]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.6854]]) tensor(0.3777, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.3749]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.3844]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.6994]]) tensor(0.3576, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.3916]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.3689]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.7317]]) tensor(1.3156, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.3475]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.3724]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.3389]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.3353]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.7544]]) tensor(0.2818, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.3209]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.3883]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.3609]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.3498]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.3589]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.3589]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.7239]]) tensor(0.3231, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.3329]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.3334]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.7572]]) tensor(0.2781, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.3406]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.2992]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.2992]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.3500]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.2476]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.9697]]) tensor(0.0308, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.2796]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.2653]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.7985]]) tensor(1.6021, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.3047]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.8787]]) tensor(0.1293, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.2918]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.2621]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.7951]]) tensor(0.2293, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.2964]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.2964]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.2573]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.2573]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.3317]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.8332]]) tensor(1.7909, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.2592]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.2592]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.7651]]) tensor(0.2678, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.3265]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.9164]]) tensor(2.4811, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.2530]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.2855]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.3626]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.3120]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.3264]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.2841]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.8800]]) tensor(2.1207, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.2686]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.2125]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.3553]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.3553]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.9056]]) tensor(2.3607, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.2843]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.6234]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.9923]]) tensor(0.0077, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.2695]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.2853]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.3515]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.2658]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.2658]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.2555]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.2661]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.2699]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.2384]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.9664]]) tensor(0.0342, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.2414]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.1973]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.8735]]) tensor(0.1352, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.2219]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.2219]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.9058]]) tensor(2.3619, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.1769]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.1408]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.1408]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.8144]]) tensor(0.2053, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.1862]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.1630]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.1530]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.1530]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.1315]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.1918]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.1992]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.9810]]) tensor(0.0192, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.1359]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.1917]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.9153]]) tensor(0.0886, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.1330]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.1917]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.9720]]) tensor(3.5742, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.1002]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.1534]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.1484]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.9644]]) tensor(0.0362, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.2145]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.2372]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.0891]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.0891]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.1361]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.9253]]) tensor(2.5949, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.1516]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.1586]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.1116]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.1374]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.8814]]) tensor(0.1262, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.2023]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.2956]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.2956]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.9887]]) tensor(4.4796, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.1523]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.2582]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.9958]]) tensor(0.0042, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.1043]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.1893]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.9684]]) tensor(0.0321, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.2175]], grad_fn=<SigmoidBackward>)\n",
            "s_pro tensor([[0.3099]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.9225]]) tensor(0.0807, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "s_pro tensor([[0.2817]], grad_fn=<SigmoidBackward>)\n",
            "c and loss tensor([[0.9924]]) tensor(0.0077, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "eval\n",
            "c_pro:tensor([[0.9993]], grad_fn=<SigmoidBackward>), pre_label:0, label:tensor([0.])\n",
            "c_pro:tensor([[0.9996]], grad_fn=<SigmoidBackward>), pre_label:0, label:tensor([1.])\n",
            "c_pro:tensor([[0.8847]], grad_fn=<SigmoidBackward>), pre_label:0, label:tensor([0.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "A0EhCzVGy9Hy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Testing functions"
      ]
    },
    {
      "metadata": {
        "id": "um3-db2Wy8P9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xUFxRjonERie",
        "colab_type": "code",
        "outputId": "d13a18a0-e2ef-4265-b731-31e4dfdb9d03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "cell_type": "code",
      "source": [
        "reward_pool =  [-1,-2,-3,-4,-5,-6,-7,-8]\n",
        "running_add = 0\n",
        "for i in reversed(range(len(reward_pool))):\n",
        "    print(i)\n",
        "    if reward_pool[i] == 0:\n",
        "         running_add = 0\n",
        "    else:\n",
        "        running_add = running_add * gamma + reward_pool[i]\n",
        "        reward_pool[i] = running_add\n",
        "        \n",
        "print(reward_pool)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7\n",
            "6\n",
            "5\n",
            "4\n",
            "3\n",
            "2\n",
            "1\n",
            "0\n",
            "[-34.35730017846292, -33.694242604508, -32.0143864692, -29.30746108, -25.563092, -20.7708, -14.92, -8.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "O6KYxGe-pHEA",
        "colab_type": "code",
        "outputId": "89ccc8a6-8669-4cd1-f4c5-c73dc69ac80d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "children = list(policy_c.children())\n",
        "for child in children:\n",
        "    print(child)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear(in_features=128, out_features=128, bias=True)\n",
            "Linear(in_features=128, out_features=128, bias=True)\n",
            "Linear(in_features=128, out_features=1, bias=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yg4eFCx_Nkz6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "\n",
        "\n",
        "# # ---- Public functions\n",
        "\n",
        "# def add_flops_counting_methods(net_main_module):\n",
        "#     \"\"\"Adds flops counting functions to an existing model. After that\n",
        "#     the flops count should be activated and the model should be run on an input\n",
        "#     image.\n",
        "    \n",
        "#     Example:\n",
        "    \n",
        "#     fcn = add_flops_counting_methods(fcn)\n",
        "#     fcn = fcn.cuda().train()\n",
        "#     fcn.start_flops_count()\n",
        "    \n",
        "#     _ = fcn(batch)\n",
        "    \n",
        "#     fcn.compute_average_flops_cost() / 1e9 / 2 # Result in GFLOPs per image in batch\n",
        "    \n",
        "#     Important: dividing by 2 only works for resnet models -- see below for the details\n",
        "#     of flops computation.\n",
        "    \n",
        "#     Attention: we are counting multiply-add as two flops in this work, because in\n",
        "#     most resnet models convolutions are bias-free (BN layers act as bias there)\n",
        "#     and it makes sense to count muliply and add as separate flops therefore.\n",
        "#     This is why in the above example we divide by 2 in order to be consistent with\n",
        "#     most modern benchmarks. For example in \"Spatially Adaptive Computatin Time for Residual\n",
        "#     Networks\" by Figurnov et al multiply-add was counted as two flops.\n",
        "    \n",
        "#     This module computes the average flops which is necessary for dynamic networks which\n",
        "#     have different number of executed layers. For static networks it is enough to run the network\n",
        "#     once and get statistics (above example).\n",
        "    \n",
        "#     Implementation:\n",
        "#     The module works by adding batch_count to the main module which tracks the sum\n",
        "#     of all batch sizes that were run through the network.\n",
        "    \n",
        "#     Also each convolutional layer of the network tracks the overall number of flops\n",
        "#     performed.\n",
        "    \n",
        "#     The parameters are updated with the help of registered hook-functions which\n",
        "#     are being called each time the respective layer is executed.\n",
        "    \n",
        "#     Parameters\n",
        "#     ----------\n",
        "#     net_main_module : torch.nn.Module\n",
        "#         Main module containing network\n",
        "        \n",
        "#     Returns\n",
        "#     -------\n",
        "#     net_main_module : torch.nn.Module\n",
        "#         Updated main module with new methods/attributes that are used\n",
        "#         to compute flops.\n",
        "#     \"\"\"\n",
        "    \n",
        "#     # adding additional methods to the existing module object,\n",
        "#     # this is done this way so that each function has access to self object\n",
        "#     net_main_module.start_flops_count = start_flops_count.__get__(net_main_module)\n",
        "#     net_main_module.stop_flops_count = stop_flops_count.__get__(net_main_module)\n",
        "#     net_main_module.reset_flops_count = reset_flops_count.__get__(net_main_module)\n",
        "#     net_main_module.compute_average_flops_cost = compute_average_flops_cost.__get__(net_main_module)\n",
        "    \n",
        "#     net_main_module.reset_flops_count()\n",
        "    \n",
        "#     # Adding varialbles necessary for masked flops computation\n",
        "#     net_main_module.apply(add_flops_mask_variable_or_reset)\n",
        "    \n",
        "#     return net_main_module\n",
        "\n",
        "\n",
        "\n",
        "# def compute_average_flops_cost(self):\n",
        "#     \"\"\"\n",
        "#     A method that will be available after add_flops_counting_methods() is called\n",
        "#     on a desired net object.\n",
        "    \n",
        "#     Returns current mean flops consumption per image.\n",
        "    \n",
        "#     \"\"\"\n",
        "    \n",
        "#     batches_count = self.__batch_counter__\n",
        "    \n",
        "#     flops_sum = 0\n",
        "    \n",
        "#     for module in self.modules():\n",
        "\n",
        "#         if isinstance(module, torch.nn.Conv2d):\n",
        "\n",
        "#             flops_sum += module.__flops__\n",
        "    \n",
        "    \n",
        "#     return flops_sum / batches_count\n",
        "\n",
        "\n",
        "# def start_flops_count(self):\n",
        "#     \"\"\"\n",
        "#     A method that will be available after add_flops_counting_methods() is called\n",
        "#     on a desired net object.\n",
        "    \n",
        "#     Activates the computation of mean flops consumption per image.\n",
        "#     Call it before you run the network.\n",
        "    \n",
        "#     \"\"\"\n",
        "    \n",
        "#     add_batch_counter_hook_function(self)\n",
        "    \n",
        "#     self.apply(add_flops_counter_hook_function)\n",
        "\n",
        "    \n",
        "# def stop_flops_count(self):\n",
        "#     \"\"\"\n",
        "#     A method that will be available after add_flops_counting_methods() is called\n",
        "#     on a desired net object.\n",
        "    \n",
        "#     Stops computing the mean flops consumption per image.\n",
        "#     Call whenever you want to pause the computation.\n",
        "    \n",
        "#     \"\"\"\n",
        "    \n",
        "#     remove_batch_counter_hook_function(self)\n",
        "    \n",
        "#     self.apply(remove_flops_counter_hook_function)\n",
        "\n",
        "    \n",
        "# def reset_flops_count(self):\n",
        "#     \"\"\"\n",
        "#     A method that will be available after add_flops_counting_methods() is called\n",
        "#     on a desired net object.\n",
        "    \n",
        "#     Resets statistics computed so far.\n",
        "    \n",
        "#     \"\"\"\n",
        "    \n",
        "#     add_batch_counter_variables_or_reset(self)\n",
        "    \n",
        "#     self.apply(add_flops_counter_variable_or_reset)\n",
        "\n",
        "\n",
        "# def add_flops_mask(module, mask):\n",
        "    \n",
        "#     def add_flops_mask_func(module):\n",
        "        \n",
        "#         if isinstance(module, torch.nn.Conv2d):\n",
        "            \n",
        "#             module.__mask__ = mask\n",
        "    \n",
        "#     module.apply(add_flops_mask_func)\n",
        "\n",
        "    \n",
        "# def remove_flops_mask(module):\n",
        "    \n",
        "#     module.apply(add_flops_mask_variable_or_reset)\n",
        "\n",
        "    \n",
        "# # ---- Internal functions\n",
        "\n",
        "\n",
        "# def conv_flops_counter_hook(conv_module, input, output):\n",
        "        \n",
        "#     # Can have multiple inputs, getting the first one\n",
        "#     input = input[0]\n",
        "    \n",
        "#     batch_size = input.shape[0]\n",
        "#     output_height, output_width = output.shape[2:]\n",
        "    \n",
        "#     kernel_height, kernel_width = conv_module.kernel_size\n",
        "#     in_channels = conv_module.in_channels\n",
        "#     out_channels = conv_module.out_channels\n",
        "    \n",
        "#     # We count multiply-add as 2 flops\n",
        "#     conv_per_position_flops = 2 * kernel_height * kernel_width * in_channels * out_channels\n",
        "    \n",
        "#     active_elements_count = batch_size * output_height * output_width\n",
        "    \n",
        "#     if conv_module.__mask__ is not None:\n",
        "        \n",
        "#         # (b, 1, h, w)\n",
        "#         flops_mask = conv_module.__mask__.expand(batch_size, 1, output_height, output_width)\n",
        "#         active_elements_count = flops_mask.sum()\n",
        "        \n",
        "    \n",
        "#     overall_conv_flops = conv_per_position_flops * active_elements_count\n",
        "      \n",
        "#     bias_flops = 0\n",
        "    \n",
        "#     if conv_module.bias is not None:\n",
        "        \n",
        "#         bias_flops = out_channels * active_elements_count\n",
        "    \n",
        "#     overall_flops = overall_conv_flops + bias_flops\n",
        "    \n",
        "#     conv_module.__flops__ += overall_flops\n",
        "\n",
        "    \n",
        "# def batch_counter_hook(module, input, output):\n",
        "    \n",
        "#     # Can have multiple inputs, getting the first one\n",
        "#     input = input[0]\n",
        "    \n",
        "#     batch_size = input.shape[0]\n",
        "    \n",
        "#     module.__batch_counter__ += batch_size\n",
        "\n",
        "\n",
        "    \n",
        "# def add_batch_counter_variables_or_reset(module):\n",
        "    \n",
        "#     module.__batch_counter__ = 0\n",
        "\n",
        "\n",
        "# def add_batch_counter_hook_function(module):\n",
        "    \n",
        "#     if hasattr(module, '__batch_counter_handle__'):\n",
        "        \n",
        "#         return\n",
        "    \n",
        "#     handle = module.register_forward_hook(batch_counter_hook)\n",
        "#     module.__batch_counter_handle__ = handle\n",
        "\n",
        "    \n",
        "# def remove_batch_counter_hook_function(module):\n",
        "    \n",
        "#     if hasattr(module, '__batch_counter_handle__'):\n",
        "        \n",
        "#         module.__batch_counter_handle__.remove()\n",
        "        \n",
        "#         del module.__batch_counter_handle__\n",
        "\n",
        "\n",
        "# def add_flops_counter_variable_or_reset(module):\n",
        "    \n",
        "#     if isinstance(module, torch.nn.Conv2d):\n",
        "        \n",
        "#         module.__flops__ = 0\n",
        "\n",
        "# def add_flops_counter_hook_function(module):\n",
        "        \n",
        "#     if isinstance(module, torch.nn.Conv2d):\n",
        "        \n",
        "#         if hasattr(module, '__flops_handle__'):\n",
        "            \n",
        "#             return\n",
        "\n",
        "#         handle = module.register_forward_hook(conv_flops_counter_hook)\n",
        "#         module.__flops_handle__ = handle\n",
        "\n",
        "# def remove_flops_counter_hook_function(module):\n",
        "    \n",
        "#     if isinstance(module, torch.nn.Conv2d):\n",
        "        \n",
        "#         if hasattr(module, '__flops_handle__'):\n",
        "            \n",
        "#             module.__flops_handle__.remove()\n",
        "            \n",
        "#             del module.__flops_handle__\n",
        "\n",
        "# # --- Masked flops counting\n",
        "\n",
        "\n",
        "# # Also being run in the initialization\n",
        "# def add_flops_mask_variable_or_reset(module):\n",
        "    \n",
        "#     if isinstance(module, torch.nn.Conv2d):\n",
        "        \n",
        "#         module.__mask__ = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xwmD_uYRNyQ3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# fcn = model\n",
        "# fcn = add_flops_counting_methods(fcn)\n",
        "# fcn = fcn.cuda().train()\n",
        "# fcn.start_flops_count()\n",
        "# _ = fcn(text_input)    \n",
        "# fcn.compute_average_flops_cost() / 1e9 / 2 # Result in GFLOPs per image in batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pGh--sFMQLEY",
        "colab_type": "code",
        "outputId": "07b8c96f-67be-48a3-c843-f5d438c8956a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "t = torch.zeros([2,1,5,1])\n",
        "print(t.shape)\n",
        "print(t.squeeze(3).shape)\n",
        "print(t.squeeze(0).shape)\n",
        "print(t.squeeze().shape)\n",
        "print(t.squeeze(1).shape)\n",
        "print(t.unsqueeze(0).shape)\n",
        "print(t.unsqueeze(1).shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 1, 5, 1])\n",
            "torch.Size([2, 1, 5])\n",
            "torch.Size([2, 1, 5, 1])\n",
            "torch.Size([2, 5])\n",
            "torch.Size([2, 5, 1])\n",
            "torch.Size([1, 2, 1, 5, 1])\n",
            "torch.Size([2, 1, 1, 5, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x_lJD0njSQR3",
        "colab_type": "code",
        "outputId": "fe70556e-6012-4660-c951-bda9ca3a7adf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "cell_type": "code",
      "source": [
        "m = Bernoulli(torch.tensor([0.4]))\n",
        "action = m.sample()\n",
        "print()\n",
        "print(action)\n",
        "print(m.logits)\n",
        "print(m.log_prob(action))\n",
        "n = Multinomial(1, torch.tensor([0.1,0.1,0.1,0.7]))\n",
        "action = n.sample()\n",
        "print()\n",
        "print(action)\n",
        "print(n.logits)\n",
        "print(n.log_prob(action))\n",
        "print()\n",
        "for i in range(5):\n",
        "    action = n.sample()\n",
        "    print(action)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "tensor([0.])\n",
            "tensor([-0.4055])\n",
            "tensor([-0.5108])\n",
            "\n",
            "tensor([0., 0., 0., 1.])\n",
            "tensor([-2.3026, -2.3026, -2.3026, -0.3567])\n",
            "tensor(-0.3567)\n",
            "\n",
            "tensor([0., 0., 0., 1.])\n",
            "tensor([0., 0., 0., 1.])\n",
            "tensor([1., 0., 0., 0.])\n",
            "tensor([0., 0., 1., 0.])\n",
            "tensor([0., 0., 0., 1.])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}